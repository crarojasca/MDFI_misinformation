{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c897c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:05:51.535042Z",
     "start_time": "2023-04-09T10:05:30.476659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import demoji\n",
    "import curlify\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAADSXjAEAAAAAQ32sCpTWcilVX%2BQj0BjOArOSYCE%3DUax6ZCizLRwwQcWQirExGQEDEB903dxBZKM4LAuGTMTvMa4jVP\"\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b87777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:05:51.538736Z",
     "start_time": "2023-04-09T10:05:51.536625Z"
    }
   },
   "outputs": [],
   "source": [
    "SCRAP_CTTS = True\n",
    "SCRAP_DENIALS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47abf26",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac1a3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:05:51.549015Z",
     "start_time": "2023-04-09T10:05:51.539934Z"
    }
   },
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "def getTweets(query, max_results=None, next_token=None):\n",
    "    # Twitter Endpoint\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    \n",
    "    # Auth Bearer no research access or premium for the moment\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "    \n",
    "    # Query parameters\n",
    "    params = {\n",
    "        'query': query,\n",
    "    #     'start_time': start_date,\n",
    "    #     'end_time': end_date,\n",
    "        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "        'next_token': {}\n",
    "    }\n",
    "    if max_results: params['max_results'] = max_results\n",
    "    if next_token: params['next_token'] = next_token\n",
    "\n",
    "    try:\n",
    "        time.sleep(6)\n",
    "        req = requests.Request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            params=params,\n",
    "            headers=headers\n",
    "        ).prepare()\n",
    "\n",
    "        response = s.send(req)\n",
    "        results = json.loads(response.text)\n",
    "        return results\n",
    "    except:\n",
    "        print(json.loads(response.text))\n",
    "\n",
    "def getUser(username):\n",
    "    # User Endpoint \n",
    "    url = \"https://api.twitter.com/2/users/by/username/\" + username\n",
    "    # Auth Bearer no research access or premium for the moment\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "        'next_token': {}\n",
    "    }\n",
    "    \n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        req = requests.Request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            params=params,\n",
    "            headers=headers\n",
    "        ).prepare()\n",
    "\n",
    "        response = s.send(req)\n",
    "        results = json.loads(response.text)[\"data\"]\n",
    "\n",
    "        return json.loads(response.text)[\"data\"]\n",
    "    except:\n",
    "        print(json.loads(response.text)[\"errors\"][0][\"detail\"])\n",
    "\n",
    "\n",
    "def count_tweets(query):\n",
    "    # User Endpoint \n",
    "    url = \"https://api.twitter.com/2/tweets/counts/recent\"\n",
    "    # Auth Bearer no research access or premium for the moment\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer {}\".format(bearer_token)\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"granularity\": \"day\"\n",
    "    }\n",
    "\n",
    "    time.sleep(1)\n",
    "    req = requests.Request(\n",
    "        \"GET\",\n",
    "        url,\n",
    "        params=params,\n",
    "        headers=headers\n",
    "    ).prepare()\n",
    "\n",
    "    response = s.send(req)\n",
    "    results = json.loads(response.text)\n",
    "    return results[\"meta\"][\"total_tweet_count\"]\n",
    "    \n",
    "    \n",
    "def extract_data(query, max_result=100):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Scrap all the data up to the last page\n",
    "    next_token = None\n",
    "\n",
    "    n_tweets = count_tweets(query)\n",
    "    if n_tweets==0:\n",
    "        return data\n",
    "    \n",
    "    #\n",
    "    pbar = tqdm(total=math.ceil(n_tweets/max_result))\n",
    "    while True:\n",
    "        pbar.update(1)\n",
    "        try:\n",
    "            results = getTweets(query=query, max_results=max_result, next_token=next_token)\n",
    "            data = pd.concat([data, pd.DataFrame(results[\"data\"])], ignore_index=True)\n",
    "            if not \"next_token\" in results[\"meta\"]:\n",
    "                break\n",
    "            next_token = results[\"meta\"][\"next_token\"] \n",
    "        except:\n",
    "            print(results)\n",
    "            break\n",
    "    pbar.close()\n",
    "    \n",
    "    # Reordering de columns\n",
    "    columns = list(data.columns)\n",
    "    columns.remove(\"text\")\n",
    "    columns.remove(\"public_metrics\")\n",
    "    data = data[[\"text\", \"public_metrics\"] + columns]\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lower\n",
    "    preprocessed_text = text.lower()\n",
    "    # Remove Handle\n",
    "    preprocessed_text = re.sub(\"@\\w+\", \"\", preprocessed_text)\n",
    "    # Remove Hashtag\n",
    "    preprocessed_text = re.sub(\"#\\w+\", \"\", preprocessed_text)\n",
    "    # Remove Links\n",
    "    preprocessed_text = re.sub(r'http[s]?:\\S+', '', preprocessed_text, flags=re.MULTILINE)\n",
    "    # Remove emotes \n",
    "    preprocessed_text = demoji.replace(preprocessed_text, \"\")\n",
    "    # Remove new line\n",
    "    preprocessed_text = re.sub(\"\\\\n\", \"\", preprocessed_text)\n",
    "    # Remove extra spaces \n",
    "    preprocessed_text = preprocessed_text.strip()\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def get_data(query):\n",
    "    results = extract_data(query)\n",
    "    results[\"preprocessed_text\"] = results[\"text\"].apply(preprocess_text)\n",
    "    results = results.sort_values(by=\"created_at\", ascending=False)\n",
    "    results = results.drop_duplicates(subset=['text'], keep='last')\n",
    "    results = results[(results.preprocessed_text!=\"\")].copy(deep=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef419342",
   "metadata": {},
   "source": [
    "## ClimateScam\n",
    "### Original Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7f904b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.537938Z",
     "start_time": "2023-04-09T10:05:51.550476Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets/counts/recent?query=from%3AClimateScam&granularity=day (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0bb473d850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    917\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f0bb473d850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets/counts/recent?query=from%3AClimateScam&granularity=day (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0bb473d850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrom:ClimateScam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m results[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublic_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(query):\n\u001b[0;32m--> 148\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m    150\u001b[0m     results \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(query, max_result)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Scrap all the data up to the last page\u001b[39;00m\n\u001b[1;32m    101\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m n_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mcount_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_tweets\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mcount_tweets\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     84\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m req \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     url,\n\u001b[1;32m     88\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     89\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders\n\u001b[1;32m     90\u001b[0m )\u001b[38;5;241m.\u001b[39mprepare()\n\u001b[0;32m---> 92\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tweet_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets/counts/recent?query=from%3AClimateScam&granularity=day (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0bb473d850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"
     ]
    }
   ],
   "source": [
    "results = get_data(\"from:ClimateScam\")\n",
    "print(results.shape)\n",
    "results[[\"preprocessed_text\", \"text\", \"public_metrics\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e3880",
   "metadata": {},
   "source": [
    "### Complete lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f710fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.540449Z",
     "start_time": "2023-04-09T10:06:13.540441Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climate_scam_handle = get_data(\"@ClimateScam OR #ClimateScam OR from:ClimateScam -is:retweet\")\n",
    "print(climate_scam_handle.shape)\n",
    "climate_scam_handle[[\"preprocessed_text\", \"text\", \"public_metrics\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd43de2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.541250Z",
     "start_time": "2023-04-09T10:06:13.541243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag = get_data(\"#ClimateScam -is:retweet\")\n",
    "print(climate_scam_hashtag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e7529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.541973Z",
     "start_time": "2023-04-09T10:06:13.541966Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag[[\"preprocessed_text\", \"text\", \"public_metrics\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531658e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.542828Z",
     "start_time": "2023-04-09T10:06:13.542821Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(col):    \n",
    "    df = col.str.split(expand=True).stack().value_counts().reset_index()\n",
    "    df.columns = ['Word', 'Frequency'] \n",
    "    df = df[~df[\"Word\"].str.lower().isin(all_stopwords)]\n",
    "    return df\n",
    "\n",
    "count_words(climate_scam_hashtag.text)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9010d",
   "metadata": {},
   "source": [
    "## CTTs\n",
    "### Extracting\n",
    "#### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c4727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.543337Z",
     "start_time": "2023-04-09T10:06:13.543330Z"
    }
   },
   "outputs": [],
   "source": [
    "ctts = pd.read_csv(\"../CTTs/ctt_twitter_handles.csv\")\n",
    "ctts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c3d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.544019Z",
     "start_time": "2023-04-09T10:06:13.544012Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(ctts.shape[0]):\n",
    "    username = ctts.loc[i].screen_name\n",
    "    user_meta = getUser(username)\n",
    "    ctts.loc[i, 'followers_count'] = user_meta['public_metrics']['followers_count']\n",
    "    ctts.loc[i, 'following_count'] = user_meta['public_metrics']['following_count']\n",
    "    ctts.loc[i, 'tweet_count'] = user_meta['public_metrics']['tweet_count']\n",
    "    ctts.loc[i, 'listed_count'] = user_meta['public_metrics']['listed_count']\n",
    "    ctts.loc[i, 'description'] = user_meta['description']\n",
    "    ctts.loc[i, 'id'] = user_meta['id']\n",
    "    ctts.loc[i, 'verified'] = user_meta['verified']\n",
    "    ctts.loc[i, 'created_at'] = user_meta['created_at']\n",
    "ctts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cd66c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.544591Z",
     "start_time": "2023-04-09T10:06:13.544584Z"
    }
   },
   "outputs": [],
   "source": [
    "## Total Tweets\n",
    "ctts.tweet_count.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c11c8",
   "metadata": {},
   "source": [
    "#### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb0e2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.545141Z",
     "start_time": "2023-04-09T10:06:13.545134Z"
    }
   },
   "outputs": [],
   "source": [
    "handles_process = ctts.screen_name.unique()\n",
    "    \n",
    "total_sum = 0\n",
    "for username in tqdm(handles_process):\n",
    "    number = count_tweets(\"@{} -is:retweet\".format(username))\n",
    "    total_sum+=number\n",
    "print(\"Number of tweets: {}\".format(total_sum))\n",
    "print(\"Aproximate time to process: {} minutes\".format(total_sum*6/60/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da1a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.545872Z",
     "start_time": "2023-04-09T10:06:13.545865Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if SCRAP_CTTS:\n",
    "    file = \"../Twitter_scrapped/CTTs/tweets_scrapped-CTTs_{}.csv\".format(today)\n",
    "    if os.path.isfile(file):\n",
    "        tweets = pd.read_csv(file)\n",
    "        handles_process = list(set(ctts.screen_name.unique()) - set(tweets.username.unique()))\n",
    "    else:\n",
    "        tweets = pd.DataFrame()\n",
    "        handles_process = ctts.screen_name.unique()\n",
    "\n",
    "    for username in tqdm(handles_process):\n",
    "        handle_tweets = extract_data(\"@{} -is:retweet\".format(username), 100)\n",
    "        handle_tweets[\"username\"] = username\n",
    "        tweets = pd.concat([tweets, handle_tweets])\n",
    "        tweets.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81801de",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1373336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.546441Z",
     "start_time": "2023-04-09T10:06:13.546433Z"
    }
   },
   "outputs": [],
   "source": [
    "file = \"../Twitter_scrapped/CTTs/tweets_scrapped-CTTs_{}.csv\".format(today)\n",
    "tweets = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea04df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.546982Z",
     "start_time": "2023-04-09T10:06:13.546975Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets[\"preprocessed_text\"] = tweets[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4318a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.547634Z",
     "start_time": "2023-04-09T10:06:13.547627Z"
    }
   },
   "outputs": [],
   "source": [
    "real_tweets = tweets[tweets.referenced_tweets.isna() & (tweets.preprocessed_text!=\"\")].copy(deep=True)\n",
    "print(\"Number of tweets: {} from {} to {}.\".format(\n",
    "    real_tweets.shape[0],\n",
    "    tweets.created_at.min(),\n",
    "    tweets.created_at.max()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a089a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.548329Z",
     "start_time": "2023-04-09T10:06:13.548322Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_words(tweets.text)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda99ef",
   "metadata": {},
   "source": [
    "## Deniers\n",
    "### Extracting\n",
    "#### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe4b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.548852Z",
     "start_time": "2023-04-09T10:06:13.548842Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denials = pd.read_csv(\"../CTTs/denier_twitter_handles.csv\")\n",
    "for i in range(denials.shape[0]):\n",
    "    username = denials.loc[i].screen_name\n",
    "    user_meta = getUser(username)\n",
    "    if not user_meta:\n",
    "        continue\n",
    "    denials.loc[i, 'exist'] = True\n",
    "    denials.loc[i, 'followers_count'] = user_meta['public_metrics']['followers_count']\n",
    "    denials.loc[i, 'following_count'] = user_meta['public_metrics']['following_count']\n",
    "    denials.loc[i, 'tweet_count'] = user_meta['public_metrics']['tweet_count']\n",
    "    denials.loc[i, 'listed_count'] = user_meta['public_metrics']['listed_count']\n",
    "    denials.loc[i, 'description'] = user_meta['description']\n",
    "    denials.loc[i, 'id'] = user_meta['id']\n",
    "    denials.loc[i, 'verified'] = user_meta['verified']\n",
    "    denials.loc[i, 'created_at'] = user_meta['created_at']\n",
    "denials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.549475Z",
     "start_time": "2023-04-09T10:06:13.549469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"{} in total {}.\".format((denials.exist==True).sum(), denials.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7148c4",
   "metadata": {},
   "source": [
    "#### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ed3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.550266Z",
     "start_time": "2023-04-09T10:06:13.550258Z"
    }
   },
   "outputs": [],
   "source": [
    "handles_process = denials[denials.exist==True].screen_name.unique()\n",
    "    \n",
    "total_sum = 0\n",
    "for username in tqdm(handles_process):\n",
    "    number = count_tweets(\"from:{} -is:retweet\".format(username))\n",
    "    total_sum+=number\n",
    "print(\"Number of tweets: {}\".format(total_sum))\n",
    "print(\"Aproximate time to process: {} minutes\".format(total_sum*6/60/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60b850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.550955Z",
     "start_time": "2023-04-09T10:06:13.550947Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denials_file = \"../Twitter_scrapped/deniers/{}.csv\".format(today)\n",
    "denials_to_process = denials[denials.exist==True]\n",
    "if SCRAP_DENIALS:\n",
    "    if os.path.isfile(denials_file):\n",
    "        tweets_denials = pd.read_csv(denials_file)\n",
    "        handles_process = list(\n",
    "            set(denials_to_process.screen_name.unique()) - set(tweets_denials.username.unique()))\n",
    "    else:\n",
    "        tweets_denials = pd.DataFrame()\n",
    "        handles_process = denials_to_process.screen_name.unique()\n",
    "\n",
    "    for username in tqdm(handles_process):\n",
    "        handle_tweets = extract_data(\"from:{} -is:retweet\".format(username), 100)\n",
    "        handle_tweets[\"username\"] = username\n",
    "        tweets_denials = pd.concat([tweets, handle_tweets])\n",
    "        tweets_denials.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab819b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.551490Z",
     "start_time": "2023-04-09T10:06:13.551479Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c439718",
   "metadata": {},
   "source": [
    "## Model\n",
    "### #ClimateScam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c1fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.552283Z",
     "start_time": "2023-04-09T10:06:13.552276Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Load and pre-process the text data\n",
    "# Define text pre-processing functions\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "def strip_underscores(text):\n",
    "    return re.sub(r'_+', ' ', text)\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "# Merge text pre-processing functions\n",
    "def denoise_text(text):\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = strip_underscores(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Pre-process the text\n",
    "climate_scam_hashtag['roberta_preprocessed'] = climate_scam_hashtag[\"text\"].astype(str).apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37021080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.552870Z",
     "start_time": "2023-04-09T10:06:13.552863Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# Define the model \n",
    "architecture = 'roberta'\n",
    "# model_name = 'CARDS_RoBERTa_Classifier'\n",
    "model_name = \"../cards/models/CARDS_RoBERTa_Classifier\"\n",
    "\n",
    "# Load the classifier\n",
    "roberta_model = ClassificationModel(architecture, model_name, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9061e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.553643Z",
     "start_time": "2023-04-09T10:06:13.553636Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions, raw_outputs = roberta_model.predict(list(climate_scam_hashtag.roberta_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc899708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.554330Z",
     "start_time": "2023-04-09T10:06:13.554323Z"
    }
   },
   "outputs": [],
   "source": [
    "le = pickle.load(open(\"../le_cards.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abce1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.554880Z",
     "start_time": "2023-04-09T10:06:13.554874Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag['roberta_pred'] = le.inverse_transform(predictions)\n",
    "climate_scam_hashtag['roberta_proba'] = [max(softmax(element[0])) for element in raw_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fae45f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.555393Z",
     "start_time": "2023-04-09T10:06:13.555387Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag.roberta_pred.value_counts().to_frame().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3548351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.555982Z",
     "start_time": "2023-04-09T10:06:13.555975Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag.roberta_pred.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec556b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.556591Z",
     "start_time": "2023-04-09T10:06:13.556584Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag[[\"text\", \"roberta_pred\", \"roberta_proba\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85b0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.557065Z",
     "start_time": "2023-04-09T10:06:13.557058Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_scam_hashtag.loc[\n",
    "    climate_scam_hashtag.roberta_pred==\"0_0\", [\"text\", \"roberta_pred\", \"roberta_proba\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128bd32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.557781Z",
     "start_time": "2023-04-09T10:06:13.557774Z"
    }
   },
   "outputs": [],
   "source": [
    "results['roberta_preprocessed'] = results[\"text\"].astype(str).apply(denoise_text)\n",
    "predictions, raw_outputs = roberta_model.predict(list(results.roberta_preprocessed))\n",
    "results['roberta_pred'] = le.inverse_transform(predictions)\n",
    "results['roberta_proba'] = [max(softmax(element[0])) for element in raw_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c42d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.558563Z",
     "start_time": "2023-04-09T10:06:13.558556Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[[\"text\", \"roberta_pred\", \"roberta_proba\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d8baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.559024Z",
     "start_time": "2023-04-09T10:06:13.559017Z"
    }
   },
   "outputs": [],
   "source": [
    "file = \"../Twitter_scrapped/@ClimateScam_CARDS_predictions_{}.csv\".format(today)\n",
    "climate_scam_hashtag.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f01a2",
   "metadata": {},
   "source": [
    "### CTTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336d478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.559858Z",
     "start_time": "2023-04-09T10:06:13.559851Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets['roberta_preprocessed'] = tweets[\"text\"].astype(str).apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c5bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.560339Z",
     "start_time": "2023-04-09T10:06:13.560332Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions, raw_outputs = roberta_model.predict(list(tweets.roberta_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b35d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.561038Z",
     "start_time": "2023-04-09T10:06:13.561019Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets['roberta_pred'] = le.inverse_transform(predictions)\n",
    "tweets['roberta_proba'] = [max(softmax(element[0])) for element in raw_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9392f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.561595Z",
     "start_time": "2023-04-09T10:06:13.561588Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv(\"@Twitter_CTTs_predictions.csv\")\n",
    "tweets.roberta_pred.value_counts().to_frame().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af6d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.562255Z",
     "start_time": "2023-04-09T10:06:13.562248Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets[tweets.referenced_tweets.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31a0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.563119Z",
     "start_time": "2023-04-09T10:06:13.563112Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets.roberta_pred.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a1b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.563643Z",
     "start_time": "2023-04-09T10:06:13.563637Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets[[\"text\", \"roberta_pred\", \"roberta_proba\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2d5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.564247Z",
     "start_time": "2023-04-09T10:06:13.564241Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets.loc[tweets.roberta_pred!=\"0_0\", [\"text\", \"roberta_pred\", \"roberta_proba\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2a3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.564847Z",
     "start_time": "2023-04-09T10:06:13.564838Z"
    }
   },
   "outputs": [],
   "source": [
    "file_CTT = \"../Twitter_scrapped/@Twitter_CTTs_predictions_{}.csv\".format(today)\n",
    "tweets.to_csv(file_CTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5fe6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.565553Z",
     "start_time": "2023-04-09T10:06:13.565540Z"
    }
   },
   "outputs": [],
   "source": [
    "# samples = (\n",
    "#     tweets.groupby(\"roberta_pred\")\n",
    "#     .sample(10, replace=True)[[\"text\", \"roberta_pred\", \"roberta_proba\"]]\n",
    "#     .drop_duplicates(\"text\")\n",
    "# )\n",
    "# samples.to_csv(\"@Twitter_CTTs_predictions_samples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f81dc2",
   "metadata": {},
   "source": [
    "### Denials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fde016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.566083Z",
     "start_time": "2023-04-09T10:06:13.566071Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials['roberta_preprocessed'] = tweets_denials[\"text\"].astype(str).apply(denoise_text)\n",
    "predictions, raw_outputs = roberta_model.predict(list(tweets_denials.roberta_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598cb03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.566715Z",
     "start_time": "2023-04-09T10:06:13.566707Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials['roberta_pred'] = le.inverse_transform(predictions)\n",
    "tweets_denials['roberta_proba'] = [max(softmax(element[0])) for element in raw_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee506ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.567316Z",
     "start_time": "2023-04-09T10:06:13.567309Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweets_denials = pd.read_csv(\"@Twitter_Denials_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31312d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.567927Z",
     "start_time": "2023-04-09T10:06:13.567920Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials.roberta_pred.value_counts().to_frame().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d64bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.568500Z",
     "start_time": "2023-04-09T10:06:13.568492Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials.roberta_pred.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ec6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.569459Z",
     "start_time": "2023-04-09T10:06:13.569448Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials[[\"text\", \"roberta_pred\", \"roberta_proba\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e7fc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.570247Z",
     "start_time": "2023-04-09T10:06:13.570237Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_denials.loc[tweets_denials.roberta_pred!=\"0_0\", [\"text\", \"roberta_pred\", \"roberta_proba\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55037ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.570880Z",
     "start_time": "2023-04-09T10:06:13.570871Z"
    }
   },
   "outputs": [],
   "source": [
    "file_denials = \"../Twitter_scrapped/@Twitter_Denials_predictions_{}.csv\".format(today)\n",
    "tweets_denials.to_csv(file_denials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4269e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T10:06:13.571615Z",
     "start_time": "2023-04-09T10:06:13.571607Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = (\n",
    "    tweets_denials.groupby(\"roberta_pred\")\n",
    "    .sample(10, replace=True)[[\"text\", \"roberta_pred\", \"roberta_proba\"]]\n",
    "    .drop_duplicates(\"text\")\n",
    ")\n",
    "samples.to_csv(\"@Twitter_Denials_predictions_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
