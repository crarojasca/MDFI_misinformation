{
    "tokenizer_name": "roberta-large",
    "model_name": "roberta-large",
    "save_name": "best-epoch",
    "output_dir": "experiments/results/hcards_complete_roberta", 
    "logging_dir": "experiments/results/logs/hcards_complete_roberta",
    "data_dir": "../datasets/third_level/train_hcards_complete.csv",
    "freeze_encoder": false, 
    "freeze_embeds": false,    
    "max_source_length": 128,
    "task_type": "sequence_matching",
    "mode": "train",
    "test_type": "dev",
    "num_labels":74,
    "do_train": true,
    "evaluation_strategy": "steps", 
    "overwrite_output_dir": true,
    "per_device_train_batch_size": 6,
    "per_device_eval_batch_size": 100,
    "eval_steps": 150,
    "save_total_limit": 3, 
    "save_steps": 150, 
    "load_best_model_at_end": true,
    "logging_steps": 150,
    "fp16": false,
    "gradient_accumulation_steps": 2,
    "num_train_epochs": 10, 
    "seed": 42,
    "warmup_steps": 1931,
    "learning_rate": 1e-05,
    "problem_type": "multi_label_classification",
    "cache_dir": "../.cache/huggingface/datasets",
    "report_to": "wandb"
}