{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d04fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T10:42:34.764621Z",
     "start_time": "2023-04-25T10:42:34.291879Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import openai\n",
    "import demoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36174505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T10:42:35.096188Z",
     "start_time": "2023-04-25T10:42:35.093804Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = \"CARDS\"\n",
    "MODEL = \"GPT-4\" ## GPT-3  CHATGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dc487e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T01:54:58.872126Z",
     "start_time": "2023-04-14T01:54:58.869886Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE = f\"../datasets/generated_disinformation_taxonomy_{DATA}_{MODEL}_specific_samples_V1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07911b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T01:55:02.236019Z",
     "start_time": "2023-04-14T01:55:00.782358Z"
    }
   },
   "outputs": [],
   "source": [
    "if DATA==\"HAMBURG\":\n",
    "    contrarian_data = pd.read_csv(\"../datasets/hamburg_misinformation_sampled.csv\", low_memory=False)\n",
    "    contrarian_data = contrarian_data[contrarian_data[\"cards_pred_score\"] > 0.9].copy(deep=True)\n",
    "elif DATA==\"CARDS\":\n",
    "    contrarian_data = pd.read_csv(\"../datasets/cards_waterloo_augmented.csv\", low_memory=False)\n",
    "    contrarian_data = contrarian_data[\n",
    "        (contrarian_data.DATASET==\"cards\")&(contrarian_data.PARTITION==\"TRAIN\")].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19076c",
   "metadata": {},
   "source": [
    "<img src=\"../images/image.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdb2781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T01:57:43.974506Z",
     "start_time": "2023-04-14T01:57:43.959270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_1</th>\n",
       "      <td>From these tweets generate a new one that implies the [ice] is not melting:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2</th>\n",
       "      <td>From these tweets generate a new tweet contrarian to climate change since we're heading into an [ice age]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_3</th>\n",
       "      <td>Contrarian tweets to climate change since the weather is [cold]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_4</th>\n",
       "      <td>Contrarian tweets to climate change since there haven't been changes in the last decades:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_6</th>\n",
       "      <td>Contrarian tweets to climate change since the sea level rise is an exageration:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_7</th>\n",
       "      <td>Contrarian tweets to climate change since extreme weather [reason_1_7]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1</th>\n",
       "      <td>Contrarian tweets to climate change since it is a natural [cycle]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_3</th>\n",
       "      <td>Contrarian tweets to climate change based on no evidence for [greenhouse effect]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1</th>\n",
       "      <td>Contrarian tweets to climate change based on [climate sensitivity is low]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2</th>\n",
       "      <td>Contrarian tweets to climate change since the [species] [aren't showing climate impacts]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_3</th>\n",
       "      <td>Contrarian tweets to climate change based on the CO2 is [beneficial]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_1</th>\n",
       "      <td>Contrarian tweets to climate change based on climate [policies are] harmful:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_2</th>\n",
       "      <td>Contrarian tweets to climate change based on climate policies are [ineffective]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_4</th>\n",
       "      <td>Contrarian tweets to climate change based on clean energy [technologies] won't work:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_5</th>\n",
       "      <td>Contrarian tweets to climate change based on the need of energy from [fossil fuels]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_1</th>\n",
       "      <td>Contrarian tweets to climate change based on climate-related [science] is [unreliable]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_2</th>\n",
       "      <td>Contrarian tweets to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  0\n",
       "1_1                                 From these tweets generate a new one that implies the [ice] is not melting:\\n\\n\n",
       "1_2  From these tweets generate a new tweet contrarian to climate change since we're heading into an [ice age]:\\n\\n\n",
       "1_3                                            Contrarian tweets to climate change since the weather is [cold]:\\n\\n\n",
       "1_4                   Contrarian tweets to climate change since there haven't been changes in the last decades:\\n\\n\n",
       "1_6                             Contrarian tweets to climate change since the sea level rise is an exageration:\\n\\n\n",
       "1_7                                     Contrarian tweets to climate change since extreme weather [reason_1_7]:\\n\\n\n",
       "2_1                                          Contrarian tweets to climate change since it is a natural [cycle]:\\n\\n\n",
       "2_3                           Contrarian tweets to climate change based on no evidence for [greenhouse effect]:\\n\\n\n",
       "3_1                                  Contrarian tweets to climate change based on [climate sensitivity is low]:\\n\\n\n",
       "3_2                   Contrarian tweets to climate change since the [species] [aren't showing climate impacts]:\\n\\n\n",
       "3_3                                       Contrarian tweets to climate change based on the CO2 is [beneficial]:\\n\\n\n",
       "4_1                                Contrarian tweets to climate change based on climate [policies are] harmful:\\n\\n\n",
       "4_2                            Contrarian tweets to climate change based on climate policies are [ineffective]:\\n\\n\n",
       "4_4                        Contrarian tweets to climate change based on clean energy [technologies] won't work:\\n\\n\n",
       "4_5                        Contrarian tweets to climate change based on the need of energy from [fossil fuels]:\\n\\n\n",
       "5_1                     Contrarian tweets to climate change based on climate-related [science] is [unreliable]:\\n\\n\n",
       "5_2    Contrarian tweets to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = {\n",
    "    \"ice\": [\"ice\", \"permafrost\", \"snow\"],\n",
    "    \"ice age\" : [\"ice age\", \"global cooling\"],\n",
    "    \"cold\": [\"cold\", \"snowing\"],\n",
    "    \"reason_1_7\": [\"isn't increasing\", \"has happened before\", \"isn't linked to climate change\"],\n",
    "    \"cycle\": [\"cycle\", \"variation\"],\n",
    "    \"greenhouse effect\": [\"greenhouse effect\", \"carbon dioxide\"],\n",
    "    \"climate sensitivity is low\": [\"climate sensitivity is low\", \"climate negative feedbacks reduce warming\"],\n",
    "    \"species\": [\"species\", \"plants\", \"reefs\"],\n",
    "    \"aren't showing climate impacts\": [\"aren't showing climate impacts\", \"are benefiting from climate change\"],\n",
    "    \"beneficial\": [\"beneficial\", \"not a pollutant\"],\n",
    "    \"policies are\": [\"policies are\", \"mitigation is\", \"adaptation is\"],\n",
    "    \"ineffective\": [\"ineffective\", \"flawed\"],\n",
    "    \"technologies\": [\"technologies\", \"biofuels\"],\n",
    "    \"fossil fuels\": [\"gas\", \"fossil fuels\", \"nuclear\"],\n",
    "    \"unreliable\": [\"unreliable\", \"uncertain\", \"unsound\"],\n",
    "    \"science\": [\"data\", \"methods\", \"models\"],\n",
    "    \"alarmists\": [\"unreliable\", \"alarmists\", \"corrupt\"]\n",
    "}\n",
    "\n",
    "queries = {\n",
    "    '1_1': \"From these tweets generate a new one that implies the [ice] is not melting:\\n\\n\",\n",
    "    '1_2': \"From these tweets generate a new tweet contrarian to climate change since we're heading into an [ice age]:\\n\\n\",\n",
    "    '1_3': \"Contrarian tweets to climate change since the weather is [cold]:\\n\\n\",\n",
    "    '1_4': \"Contrarian tweets to climate change since there haven't been changes in the last decades:\\n\\n\",\n",
    "    '1_6': \"Contrarian tweets to climate change since the sea level rise is an exageration:\\n\\n\",\n",
    "    '1_7': \"Contrarian tweets to climate change since extreme weather [reason_1_7]:\\n\\n\",\n",
    "    '2_1': \"Contrarian tweets to climate change since it is a natural [cycle]:\\n\\n\",\n",
    "    '2_3': \"Contrarian tweets to climate change based on no evidence for [greenhouse effect]:\\n\\n\",\n",
    "    '3_1': \"Contrarian tweets to climate change based on [climate sensitivity is low]:\\n\\n\",\n",
    "    '3_2': \"Contrarian tweets to climate change since the [species] [aren't showing climate impacts]:\\n\\n\",\n",
    "    '3_3': \"Contrarian tweets to climate change based on the CO2 is [beneficial]:\\n\\n\",\n",
    "    '4_1': \"Contrarian tweets to climate change based on climate [policies are] harmful:\\n\\n\",\n",
    "    '4_2': \"Contrarian tweets to climate change based on climate policies are [ineffective]:\\n\\n\",\n",
    "    '4_4': \"Contrarian tweets to climate change based on clean energy [technologies] won't work:\\n\\n\",\n",
    "    '4_5': \"Contrarian tweets to climate change based on the need of energy from [fossil fuels]:\\n\\n\",\n",
    "    '5_1': \"Contrarian tweets to climate change based on climate-related [science] is [unreliable]:\\n\\n\",\n",
    "    '5_2': \"Contrarian tweets to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n\",\n",
    "}\n",
    "pd.DataFrame(queries.values(), index=queries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648deed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T02:01:56.969395Z",
     "start_time": "2023-04-14T02:01:56.955190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_1</th>\n",
       "      <td>From these claims generate a new one that implies the [ice] is not melting:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument we're heading into an [ice age]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_3</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the [cold] weather of the moment:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_4</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument that there haven't been changes or effects in the last decades:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_6</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument that the sea level rise has been exaggerated:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_7</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument that extreme weather [reason_1_7]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument that it is a natural [cycle]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_3</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on no evidence for [greenhouse effect]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on [climate sensitivity is low]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the argument that since [species] [aren't showing climate impacts]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_3</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the CO2 is [beneficial]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_1</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on climate [policies are] harmful:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_2</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on climate policies are [ineffective]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_4</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on clean energy [technologies] won't work:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_5</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the need of energy from [fossil fuels]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_1</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on climate-related [science] is [unreliable]:\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_2</th>\n",
       "      <td>From these claims generate a new claim contrarian to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                 0\n",
       "1_1                                                                                From these claims generate a new one that implies the [ice] is not melting:\\n\\n\n",
       "1_2                                 From these claims generate a new claim contrarian to climate change based on the argument we're heading into an [ice age]:\\n\\n\n",
       "1_3                                             From these claims generate a new claim contrarian to climate change based on the [cold] weather of the moment:\\n\\n\n",
       "1_4  From these claims generate a new claim contrarian to climate change based on the argument that there haven't been changes or effects in the last decades:\\n\\n\n",
       "1_6                    From these claims generate a new claim contrarian to climate change based on the argument that the sea level rise has been exaggerated:\\n\\n\n",
       "1_7                               From these claims generate a new claim contrarian to climate change based on the argument that extreme weather [reason_1_7]:\\n\\n\n",
       "2_1                                    From these claims generate a new claim contrarian to climate change based on the argument that it is a natural [cycle]:\\n\\n\n",
       "2_3                                          From these claims generate a new claim contrarian to climate change based on no evidence for [greenhouse effect]:\\n\\n\n",
       "3_1                                                 From these claims generate a new claim contrarian to climate change based on [climate sensitivity is low]:\\n\\n\n",
       "3_2           From these claims generate a new claim contrarian to climate change based on the argument that since [species] [aren't showing climate impacts]:\\n\\n\n",
       "3_3                                                      From these claims generate a new claim contrarian to climate change based on the CO2 is [beneficial]:\\n\\n\n",
       "4_1                                               From these claims generate a new claim contrarian to climate change based on climate [policies are] harmful:\\n\\n\n",
       "4_2                                           From these claims generate a new claim contrarian to climate change based on climate policies are [ineffective]:\\n\\n\n",
       "4_4                                       From these claims generate a new claim contrarian to climate change based on clean energy [technologies] won't work:\\n\\n\n",
       "4_5                                       From these claims generate a new claim contrarian to climate change based on the need of energy from [fossil fuels]:\\n\\n\n",
       "5_1                                    From these claims generate a new claim contrarian to climate change based on climate-related [science] is [unreliable]:\\n\\n\n",
       "5_2                   From these claims generate a new claim contrarian to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = {\n",
    "    '1_1': \"From these claims generate a new one that implies the [ice] is not melting:\\n\\n\",\n",
    "    '1_2': \"From these claims generate a new claim contrarian to climate change based on the argument we're heading into an [ice age]:\\n\\n\",\n",
    "    '1_3': \"From these claims generate a new claim contrarian to climate change based on the [cold] weather of the moment:\\n\\n\",\n",
    "    '1_4': \"From these claims generate a new claim contrarian to climate change based on the argument that there haven't been changes or effects in the last decades:\\n\\n\",\n",
    "    '1_6': \"From these claims generate a new claim contrarian to climate change based on the argument that the sea level rise has been exaggerated:\\n\\n\",\n",
    "    '1_7': \"From these claims generate a new claim contrarian to climate change based on the argument that extreme weather [reason_1_7]:\\n\\n\",\n",
    "    '2_1': \"From these claims generate a new claim contrarian to climate change based on the argument that it is a natural [cycle]:\\n\\n\",\n",
    "    '2_3': \"From these claims generate a new claim contrarian to climate change based on no evidence for [greenhouse effect]:\\n\\n\",\n",
    "    '3_1': \"From these claims generate a new claim contrarian to climate change based on [climate sensitivity is low]:\\n\\n\",\n",
    "    '3_2': \"From these claims generate a new claim contrarian to climate change based on the argument that since [species] [aren't showing climate impacts]:\\n\\n\",\n",
    "    '3_3': \"From these claims generate a new claim contrarian to climate change based on the CO2 is [beneficial]:\\n\\n\",\n",
    "    '4_1': \"From these claims generate a new claim contrarian to climate change based on climate [policies are] harmful:\\n\\n\",\n",
    "    '4_2': \"From these claims generate a new claim contrarian to climate change based on climate policies are [ineffective]:\\n\\n\",\n",
    "    '4_4': \"From these claims generate a new claim contrarian to climate change based on clean energy [technologies] won't work:\\n\\n\",\n",
    "    '4_5': \"From these claims generate a new claim contrarian to climate change based on the need of energy from [fossil fuels]:\\n\\n\",\n",
    "    '5_1': \"From these claims generate a new claim contrarian to climate change based on climate-related [science] is [unreliable]:\\n\\n\",\n",
    "    '5_2': \"From these claims generate a new claim contrarian to climate change based on the fact that of climate change supporters are [alarmists]:\\n\\n\",\n",
    "}\n",
    "pd.DataFrame(queries.values(), index=queries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154b5f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T01:55:04.519385Z",
     "start_time": "2023-04-14T01:55:04.511800Z"
    }
   },
   "outputs": [],
   "source": [
    "# queries = {\n",
    "#     '4_5': \"From these claims generate a new claim contrarian to climate change based on the need of energy from [fossil fuels]:\\n\\n\",\n",
    "# }\n",
    "\n",
    "def generate_prompt(data, label, nshots=3):\n",
    "    \"\"\"Generates\"\"\"\n",
    "    # Generating fewshots\n",
    "    \n",
    "    texts = data.loc[data.claim==label, \"text\"]\n",
    "    samples = texts.sample(nshots)\n",
    "    idx = samples.index.tolist()\n",
    "    samples = samples.tolist()\n",
    "\n",
    "    samples = [\"{}. {}\".format(\n",
    "        i+1, unidecode(str(sample).replace(\"\\n\", \"\"))) for i, sample in enumerate(samples)]\n",
    "    fewshots = \"\\n\\n\".join(samples) + \"\\n\\n4. \"\n",
    "    \n",
    "    # Generating query\n",
    "    query = queries[label]\n",
    "    matches = re.findall(r\"\\[.*?\\]\", query)\n",
    "    for m in matches:\n",
    "        choice = random.choice(masks[m[1:-1]])\n",
    "        query = query.replace(m, choice)\n",
    "    \n",
    "    prompt = query + fewshots\n",
    "    \n",
    "    return prompt, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc8eea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T05:54:45.676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6a23cbe1e443848e748a9e4480e437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a42ab76a33e436cb1910bb97e4768ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_1::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34088e77af9245e7906c9f214a10b073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_2::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f76b983521440e4b6cfc4e60d794bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_3::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2b21fb8cb84b83863d68d08666bdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_4::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0d25a60bd545fba75930a76a35292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_6::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Exception: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec109d18084446e9df8d205999a7131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_7::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220b85c3725845618ffe01306208cdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_1::   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.isfile(FILE):\n",
    "    new_data = pd.read_csv(FILE)\n",
    "else:\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "n = 400\n",
    "labels = list(queries.keys())\n",
    "\n",
    "for label in tqdm(labels[6:]):\n",
    "    for i in tqdm(range(n), desc=f\"{label}:\"):       \n",
    "        sleep(0.01)\n",
    "        prompt, idx = generate_prompt(contrarian_data, label)\n",
    "        try:\n",
    "            if MODEL==\"GPT-3\":\n",
    "                prompts = [prompt]\n",
    "                response = openai.Completion.create(\n",
    "                  model=\"text-davinci-003\",\n",
    "                  prompt=prompts,\n",
    "                  temperature=0,\n",
    "                  max_tokens=60,\n",
    "                )\n",
    "                completions = [r[\"text\"] for r in response[\"choices\"]]\n",
    "            elif MODEL==\"CHATGPT\":\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=60,\n",
    "                )\n",
    "                completions = [r[\"message\"][\"content\"] for r in response[\"choices\"]]\n",
    "            elif MODEL==\"GPT-4\":\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=60,\n",
    "                )\n",
    "                completions = [r[\"message\"][\"content\"] for r in response[\"choices\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}.\")\n",
    "            sleep(60)\n",
    "        \n",
    "        tmp = {\"text\": completions, \"generated_label\": label, \"based_claims\": str(idx)}\n",
    "        \n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        new_data = pd.concat([new_data, tmp])\n",
    "        \n",
    "        new_data.to_csv(FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11859f33",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T05:54:45.677Z"
    }
   },
   "outputs": [],
   "source": [
    "new_data.drop_duplicates([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2a4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c932ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T10:44:41.421532Z",
     "start_time": "2023-04-25T10:44:39.582074Z"
    }
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124msummarize the next text:\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mThe \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mextit\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mcurse of dimensionality} in reinforcement learning is a well-known challenge, where the number of parameters to learn increases exponentially with the size of the problem \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcite\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbarto2003recent, botvinick2012hierarchical}. To overcome this problem, researchers have employed time abstraction, which involves dividing the main problem into subtasks that can be reused in different contexts and times. In robotics, for example, tasks like manipulation and grasping \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcite\u001b[39m\u001b[38;5;132;01m{gupta2019relay}\u001b[39;00m\u001b[38;5;124m require managing multiple movements depending on the main objective. However, a grasping movement is complex and can vary greatly with slight modifications to the task, such as grasping a ball or a cube. Hierarchical Reinforcement Learning (HRL) aims to abstract knowledge of the subtasks involved in the movement to facilitate transfer learning in similar scenarios. HRL involves the use of a hierarchy of policies, where high-level policies select subtasks to be performed, and lower-level policies determine how to execute these subtasks. HRL can help address the curse of dimensionality by reducing the number of states and actions that need to be learned, allowing for more efficient learning in large and complex environments.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m completions \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/uni/lib/python3.8/site-packages/openai/api_requestor.py:679\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    680\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    681\u001b[0m     )\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Complete the next idea:\n",
    "The algorithm will be developed in PyTorch, a library for Parallel Linear Programming that supports GPU usage. Most of the frameworks that have been mentioned except LCP, have already been implemented in PyTorch and can be found in public repositories. The main goal of this stage is to combine these frameworks into a standard pattern that works with the environments.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=60,\n",
    "                )\n",
    "completions = [r[\"message\"][\"content\"] for r in response[\"choices\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47ffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
