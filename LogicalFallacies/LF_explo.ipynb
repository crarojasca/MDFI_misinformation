{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05af18e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-12T00:24:43.609632Z",
     "start_time": "2022-12-12T00:24:39.093986Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from transformers.models.electra.modeling_electra import ElectraClassificationHead\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from typing import Optional\n",
    "import sys\n",
    "import os\n",
    "from logical_fallacy.codes_for_models.finetune.util import *\n",
    "from logical_fallacy.codes_for_models.finetune.evaluate import *\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    ElectraTokenizerFast,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d2b7f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-12T00:31:50.136050Z",
     "start_time": "2022-12-12T00:31:50.114113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"logical_fallacy/data/\"\n",
    "train = pd.read_csv(path + \"climate_train_mh.csv\")\n",
    "dev = pd.read_csv(path + \"climate_dev_mh.csv\")\n",
    "test = pd.read_csv(path + \"climate_test_mh.csv\")\n",
    "train.shape[0] + dev.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224d36cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:34.339319Z",
     "start_time": "2022-11-27T09:57:34.324920Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    task_type: str = field(\n",
    "        metadata={\"help\": \"Task type, can be either generation or classification\"}\n",
    "    )\n",
    "    num_labels: str = field(\n",
    "        metadata={\"help\": \"Number of labels, used for sequence classification\"}\n",
    "    )\n",
    "    mode: str = field(\n",
    "        metadata={\"help\": \"mode, can be either train or test\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    freeze_encoder: bool = field(default=False, metadata={\"help\": \"Whether tp freeze the encoder.\"})\n",
    "    freeze_embeds: bool = field(default=False, metadata={\"help\": \"Whether  to freeze the embeddings.\"})\n",
    "        \n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n",
    "    )\n",
    "    test_type: Optional[str] = field(\n",
    "        default=\"test\", metadata={\"help\": \"The type_path of the test file, test.seen, test.unseen etc.\"}\n",
    "    )\n",
    "    task: Optional[str] = field(\n",
    "        default=\"summarization\",\n",
    "        metadata={\"help\": \"Task name, summarization (or summarization_{dataset} for pegasus) or translation\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=64,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=64,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded. \"\n",
    "            \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "            \"during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    test_max_target_length: Optional[int] = field(\n",
    "        default=300,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for test target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    n_train: Optional[int] = field(default=None, metadata={\"help\": \"# training examples. None means use all.\"})\n",
    "    n_val: Optional[int] = field(default=None, metadata={\"help\": \"# validation examples. None means use all.\"})\n",
    "    n_test: Optional[int] = field(default=None, metadata={\"help\": \"# test examples. None means use all.\"})\n",
    "    eval_beams: Optional[int] = field(default=None, metadata={\"help\": \"# num_beams to use for evaluation.\"})\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"If only pad tokens should be ignored. This assumes that `config.pad_token_id` is defined.\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class EvalArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to the evaluation of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    decode: Optional[str] = field(\n",
    "        default='beam_search', metadata={\"help\": \"Decoding method used, take in value of beam_search, nucleus\"}\n",
    "    )\n",
    "    metric: Optional[str] = field(\n",
    "        default='bleu', metadata={\"help\": \"The metric used to evaluate the model, takes in value of bleu, rouge, meteor etc\"}\n",
    "    )\n",
    "    compute_metric: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": \"whether to compute metrics while generating the outputs, must be False if num_samples > 1\"}\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=5, metadata={\"help\": \"beam size used to decode\"}\n",
    "    )\n",
    "    num_samples: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"Number of decoded sequence for each input\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47322ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:34.351246Z",
     "start_time": "2022-11-27T09:57:34.340626Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, EvalArguments, Seq2SeqTrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2cf4956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:35.641808Z",
     "start_time": "2022-11-27T09:57:34.352880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uni/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model_args, data_args, eval_args, training_args = parser.parse_json_file(json_file=\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d14556c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:35.668204Z",
     "start_time": "2022-11-27T09:57:35.643380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': 'LF_train/results/fallacy/propa/deberta',\n",
       " 'overwrite_output_dir': True,\n",
       " 'do_train': True,\n",
       " 'do_eval': True,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'per_device_eval_batch_size': 128,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 1e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 5,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 1931,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'passive',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': 'LF_train/results/fallacy/propa/logs',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': False,\n",
       " 'logging_steps': 300,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 30000000000,\n",
       " 'save_total_limit': 1,\n",
       " 'save_on_each_node': False,\n",
       " 'no_cuda': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': -1,\n",
       " 'xpu_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': 300,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': 'LF_train/results/fallacy/propa/deberta',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': True,\n",
       " 'metric_for_best_model': 'loss',\n",
       " 'greater_is_better': False,\n",
       " 'ignore_data_skip': False,\n",
       " 'sharded_ddp': [],\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_HF: 'adamw_hf'>,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': ['tensorboard', 'wandb'],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'sortish_sampler': False,\n",
       " 'predict_with_generate': False,\n",
       " 'generation_max_length': None,\n",
       " 'generation_num_beams': None,\n",
       " '_n_gpu': 0,\n",
       " '__cached__setup_devices': device(type='cpu')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99891ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:40.167771Z",
     "start_time": "2022-11-27T09:57:35.671042Z"
    }
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name,\n",
    "    cache_dir=model_args.cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df45c76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T11:47:57.633775Z",
     "start_time": "2022-11-28T11:47:57.629853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/deberta-base'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0de10e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T11:47:24.703307Z",
     "start_time": "2022-11-28T11:47:22.434944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json\n",
      "loading file merges.txt from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/crarojasca/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b67b1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.217698Z",
     "start_time": "2022-11-27T09:57:41.474476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if model_args.task_type == 'generation':\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )\n",
    "else:\n",
    "    if model_args.model_name != 'microsoft/DialogRPT-updown' and config.num_labels is None:\n",
    "        config.num_labels = int(model_args.num_labels)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154a3d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.227562Z",
     "start_time": "2022-11-27T09:57:43.221028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get datasets\n",
    "train_dataset = Seq2SeqDataset(\n",
    "    tokenizer,\n",
    "    type_path=\"train\",\n",
    "    task_type = model_args.task_type,\n",
    "    mode = model_args.mode,\n",
    "    data_dir=data_args.data_dir,\n",
    "    n_obs=data_args.n_train,\n",
    "    max_target_length=data_args.max_target_length,\n",
    "    max_source_length=data_args.max_source_length,\n",
    "    prefix=model.config.prefix or \"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50422f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.246385Z",
     "start_time": "2022-11-27T09:57:43.233853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logical_fallacy/data/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4aafe6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.256741Z",
     "start_time": "2022-11-27T09:57:43.249871Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = Seq2SeqDataset(\n",
    "    tokenizer,\n",
    "    type_path=\"dev\",\n",
    "    task_type = model_args.task_type,\n",
    "    mode = model_args.mode,\n",
    "    data_dir=data_args.data_dir,\n",
    "    n_obs=data_args.n_val,\n",
    "    max_target_length=data_args.val_max_target_length,\n",
    "    max_source_length=data_args.max_source_length,\n",
    "    prefix=model.config.prefix or \"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2327248a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.268791Z",
     "start_time": "2022-11-27T09:57:43.259649Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "        Seq2SeqDataset(\n",
    "            tokenizer,\n",
    "            type_path=data_args.test_type,\n",
    "            task_type = model_args.task_type,\n",
    "            mode = model_args.mode,\n",
    "            data_dir=data_args.data_dir,\n",
    "            n_obs=data_args.n_test,\n",
    "            max_target_length=data_args.test_max_target_length,\n",
    "            max_source_length=data_args.max_source_length,\n",
    "            prefix=model.config.prefix or \"\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b6e5ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.866743Z",
     "start_time": "2022-11-27T09:57:43.272842Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset if training_args.do_predict else eval_dataset,\n",
    "    data_collator=Seq2SeqDataCollator(tokenizer, config.decoder_start_token_id,model_args.task_type, model_args.mode, data_args),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685806b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.873438Z",
     "start_time": "2022-11-27T09:57:43.869330Z"
    }
   },
   "outputs": [],
   "source": [
    "# if model_args.mode == 'train':\n",
    "#     check_output_dir(training_args)#check if output_dir exists and raises error if it exists over_wirte=False\n",
    "\n",
    "#     set_seed(training_args.seed)#set training seed\n",
    "#     if model_args.freeze_embeds:\n",
    "#         freeze_embeds(model)\n",
    "#     if model_args.freeze_encoder:\n",
    "#         freeze_params(model.get_encoder())\n",
    "#         assert_all_frozen(model.get_encoder())\n",
    "#     trainer.train()\n",
    "#     trainer.save_model(Path(training_args.output_dir).joinpath(\"best-epoch\"))#save best epoch\n",
    "\n",
    "# elif model_args.mode == 'output_prob':\n",
    "#     out_prob(model, tokenizer, trainer.get_test_dataloader(test_dataset), Path(training_args.output_dir).joinpath(\"prob.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17767f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:43.888016Z",
     "start_time": "2022-11-27T09:57:43.876108Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba19728a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:45.095810Z",
     "start_time": "2022-11-27T09:57:43.891068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/saved_models/electra-logicclimate/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/saved_models/electra-logicclimate\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30531\n",
      "}\n",
      "\n",
      "loading weights file /home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/saved_models/electra-logicclimate/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at /home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/saved_models/electra-logicclimate.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "    '/home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/saved_models/electra-logicclimate', \n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac8d7f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:34:59.315615Z",
     "start_time": "2022-11-28T01:34:59.313441Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset.src_file = \"./logical_fallacy/data/test.source\"\n",
    "test_dataset.tgt_file = \"./logical_fallacy/data/test.target\"\n",
    "test_dataset.len_file = 1350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "011a823a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:01:54.147309Z",
     "start_time": "2022-11-28T01:01:54.144272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<logical_fallacy.codes_for_models.finetune.util.Seq2SeqDataset at 0x7fc83e7dbdf0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9463b705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:01:19.297020Z",
     "start_time": "2022-11-28T01:01:19.293785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<logical_fallacy.codes_for_models.finetune.util.Seq2SeqDataset at 0x7fc83e7dbdf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5876e413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:35:01.498003Z",
     "start_time": "2022-11-28T01:35:01.355890Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "empty source line for index 1351",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test_dataset)\n",
      "File \u001b[0;32m~/Monash/MDFI_misinformation/logical_fallacy/codes_for_models/finetune/util.py:109\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(source_line, tgt_line)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m source_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty source line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tgt_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty tgt line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: tgt_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "\u001b[0;31mAssertionError\u001b[0m: empty source line for index 1351"
     ]
    }
   ],
   "source": [
    "for batch in test_dataset:\n",
    "    print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1755df32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:58:24.737559Z",
     "start_time": "2022-11-27T09:58:24.438297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "empty source line for index 1351",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs)\n",
      "File \u001b[0;32m~/Monash/MDFI_misinformation/logical_fallacy/codes_for_models/finetune/util.py:109\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(source_line, tgt_line)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m source_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty source line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tgt_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty tgt line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: tgt_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "\u001b[0;31mAssertionError\u001b[0m: empty source line for index 1351"
     ]
    }
   ],
   "source": [
    "for inputs in tqdm(list(test_dataset)):\n",
    "    print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58599d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T09:57:53.848362Z",
     "start_time": "2022-11-27T09:57:52.794653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "empty source line for index 1351",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(inputs)\n",
      "File \u001b[0;32m~/Monash/MDFI_misinformation/logical_fallacy/codes_for_models/finetune/util.py:109\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(source_line, tgt_line)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m source_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty source line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tgt_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty tgt line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: tgt_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "\u001b[0;31mAssertionError\u001b[0m: empty source line for index 1351"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs in tqdm(list(test_dataset)):\n",
    "        print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "046a8892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T22:29:11.366487Z",
     "start_time": "2022-11-24T22:29:10.919451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "empty source line for index 1351",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      3\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      4\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/Monash/MDFI_misinformation/logical_fallacy/codes_for_models/finetune/util.py:108\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    106\u001b[0m tgt_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_prefix \u001b[38;5;241m+\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetline(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_file), index)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(source_line, tgt_line)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m source_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty source line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tgt_line, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty tgt line for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: tgt_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "\u001b[0;31mAssertionError\u001b[0m: empty source line for index 1351"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs in tqdm(list(test_dataset)):\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items() if k!='labels'}\n",
    "        outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "433c223b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T22:17:34.959087Z",
     "start_time": "2022-11-24T22:17:34.957081Z"
    }
   },
   "outputs": [],
   "source": [
    "import linecache\n",
    "\n",
    "test_dataset.src_file = \"./logical_fallacy/data/test.source\"\n",
    "test_dataset.tgt_file = \"./logical_fallacy/data/test.target\"\n",
    "test_dataset.len_file = \"./logical_fallacy/data/test.len\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb04ad50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T15:32:58.692740Z",
     "start_time": "2022-11-24T15:32:58.682769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_file': PosixPath('logical_fallacy/data/dev.source'),\n",
       " 'tgt_file': PosixPath('logical_fallacy/data/dev.target'),\n",
       " 'len_file': PosixPath('logical_fallacy/data/dev.len'),\n",
       " 'src_lens': [127,\n",
       "  150,\n",
       "  150,\n",
       "  108,\n",
       "  270,\n",
       "  120,\n",
       "  70,\n",
       "  297,\n",
       "  275,\n",
       "  333,\n",
       "  74,\n",
       "  99,\n",
       "  122,\n",
       "  110,\n",
       "  38,\n",
       "  73,\n",
       "  129,\n",
       "  56,\n",
       "  122,\n",
       "  374,\n",
       "  89,\n",
       "  173,\n",
       "  326,\n",
       "  128,\n",
       "  141,\n",
       "  104,\n",
       "  209,\n",
       "  208,\n",
       "  341,\n",
       "  120,\n",
       "  268,\n",
       "  235,\n",
       "  144,\n",
       "  170,\n",
       "  193,\n",
       "  172,\n",
       "  75,\n",
       "  311,\n",
       "  229,\n",
       "  329,\n",
       "  351,\n",
       "  72,\n",
       "  295,\n",
       "  89,\n",
       "  91,\n",
       "  133,\n",
       "  249,\n",
       "  240,\n",
       "  237,\n",
       "  438,\n",
       "  281,\n",
       "  169,\n",
       "  119,\n",
       "  223,\n",
       "  171,\n",
       "  154,\n",
       "  203,\n",
       "  163,\n",
       "  83,\n",
       "  87,\n",
       "  141,\n",
       "  172,\n",
       "  131,\n",
       "  114,\n",
       "  245,\n",
       "  125,\n",
       "  166,\n",
       "  232,\n",
       "  432,\n",
       "  179,\n",
       "  369,\n",
       "  315,\n",
       "  284,\n",
       "  174,\n",
       "  143,\n",
       "  220,\n",
       "  305,\n",
       "  422,\n",
       "  469,\n",
       "  115,\n",
       "  311,\n",
       "  187,\n",
       "  253,\n",
       "  138,\n",
       "  337,\n",
       "  197,\n",
       "  190,\n",
       "  109,\n",
       "  250,\n",
       "  287,\n",
       "  124,\n",
       "  208,\n",
       "  141,\n",
       "  158,\n",
       "  275,\n",
       "  406,\n",
       "  139,\n",
       "  253,\n",
       "  201,\n",
       "  385,\n",
       "  222,\n",
       "  165,\n",
       "  367,\n",
       "  217,\n",
       "  129,\n",
       "  393,\n",
       "  274,\n",
       "  153,\n",
       "  202,\n",
       "  198,\n",
       "  50,\n",
       "  307,\n",
       "  158,\n",
       "  390,\n",
       "  204,\n",
       "  175,\n",
       "  79,\n",
       "  254,\n",
       "  124,\n",
       "  197,\n",
       "  144,\n",
       "  116,\n",
       "  96,\n",
       "  227,\n",
       "  183,\n",
       "  185,\n",
       "  239,\n",
       "  256,\n",
       "  230,\n",
       "  91,\n",
       "  236,\n",
       "  142,\n",
       "  138,\n",
       "  315,\n",
       "  97,\n",
       "  249,\n",
       "  242,\n",
       "  49,\n",
       "  140,\n",
       "  296,\n",
       "  112,\n",
       "  175,\n",
       "  41,\n",
       "  38,\n",
       "  28,\n",
       "  39,\n",
       "  61,\n",
       "  790,\n",
       "  122,\n",
       "  131,\n",
       "  115,\n",
       "  88,\n",
       "  191,\n",
       "  49,\n",
       "  47,\n",
       "  44,\n",
       "  381,\n",
       "  228,\n",
       "  338,\n",
       "  216,\n",
       "  221,\n",
       "  517,\n",
       "  208,\n",
       "  371,\n",
       "  223,\n",
       "  79,\n",
       "  97,\n",
       "  129,\n",
       "  53,\n",
       "  74,\n",
       "  124,\n",
       "  247,\n",
       "  227,\n",
       "  179,\n",
       "  95,\n",
       "  42,\n",
       "  264,\n",
       "  121,\n",
       "  27,\n",
       "  337,\n",
       "  234,\n",
       "  189,\n",
       "  62,\n",
       "  189,\n",
       "  169,\n",
       "  181,\n",
       "  266,\n",
       "  117,\n",
       "  198,\n",
       "  187,\n",
       "  172,\n",
       "  300,\n",
       "  37,\n",
       "  212,\n",
       "  139,\n",
       "  137,\n",
       "  166,\n",
       "  206,\n",
       "  342,\n",
       "  313,\n",
       "  455,\n",
       "  299,\n",
       "  184,\n",
       "  125,\n",
       "  295,\n",
       "  472,\n",
       "  338,\n",
       "  232,\n",
       "  158,\n",
       "  328,\n",
       "  177,\n",
       "  173,\n",
       "  83,\n",
       "  308,\n",
       "  1659,\n",
       "  120,\n",
       "  1296,\n",
       "  140],\n",
       " 'used_char_len': True,\n",
       " 'task_type': 'sequence_matching',\n",
       " 'max_source_length': 128,\n",
       " 'max_target_length': 300,\n",
       " 'tokenizer': PreTrainedTokenizerFast(name_or_path='microsoft/deberta-base', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)}),\n",
       " 'source_prefix': '',\n",
       " 'target_prefix': '',\n",
       " 'pad_token_id': 0,\n",
       " 'dataset_kwargs': {'mode': 'train', 'prefix': ''}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b18f4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T22:17:40.272894Z",
     "start_time": "2022-11-24T22:17:40.270473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the text has the logical fallacy of cherry-picking </s> green activists are at war with the greatest american foe since the axis powers  or so they say . the latest democratic party platform compares the fight against global warming to world war ii . using terms such as \" battlefield , \" \" siege , \" and \" front , \" those opposed this \" war effort \" have been labeled anything from nazis to holocaust deniers . ( i personally have been called a sociopath by climate activist joe romm of the center for american progress , another story . ) the upcoming election has inspired dire concern . do nt  vote for climate catastrophe  warned a washington post editorialist .  at this point ,  stated michael klare , professor of peace and world security at hampshire college ,  electing green - minded leaders , stopping climate deniers ( or ignorers ) from capturing high office , and opposing fossil -fueled ultranationalism is the only realistic path to a habitable planet .  desperation backfire ? equating the third reich with the free society s fossil - fuel reliance , and charging republicans with climate destruction , is from the theater of the absurd . americans care greatly about the future ; to say otherwise is to deny their very humanity . it is right and fair that critics of climate catastrophism reject calls for bigger government , tax - wise and regulation - wise . and the great middle can be excused for realizing that obsessing about climate change is avoiding a frank discussion about the here - and - now problems of budget deficits , the federal debt , school choice , entitlement reform , and so on . perhaps there is good news in the ugliness of desperate activists who are trying to get their issue out front . not only do polls suggest the public is unmoved at home and in abroad , serial exaggeration at this point is arguably backfiring , confirming the perils of climate exaggeration . until civility returns , our side can ask whether the critics are acting like the gestapo of global warming . two can play a game that should not be played at all . no point of no return global warming activists tout their commitment to evidence and reason . yet their dismal track record seems to lead to only more drama and hyperbole , not humility and open - mindedness . back in 2006 , al gore prophesied that unless the world dramatically reduced greenhouse gasses , we would hit a \" point of no return . \" and in his book review of gore s book and movie from that year , an inconvenient truth , scientist james hansen unequivocally stated :  we have at most ten years  not ten years to decide upon action , but ten years to alter fundamentally the trajectory of global greenhouse emissions .  time is up on gore s  point of no return  and hansen s  critical tipping point .  but the two fathers of the global - warming movement ( hansen and gore got it going back in the summer of 1988 ) have nary admitted their exaggeration nor set a new timetable for effective action . many others have followed the same scare - and - hide pattern . rajendra pachauri , while head of a united nations climate panel , pleaded that without drastic action before 2012 , it would be too late to save the planet . back in the late 1980s , the un claimed that if global warming were not checked by 2000 , rising sea levels would wash entire counties away . four years ago , peter wadhams , professor of ocean physics at the university of cambridge , predicted  global disaster  from the demise of arctic sea ice  in four years . he too , is eating crow . there is some levity in all this charade . in 2009 , then - british prime minister gordon brown predicted that the world had only 50 days to save the planet from global warming . but fifty days and years later , and the earth still spins . science diminished falsified and sure - to - be - falsified exaggerations from a parade of ph.d. scientists are ruining the reputation of science itself . physical science has turned into profit - maximizing political science . as judith curry explained to congress : in their efforts to promote their  cause ,  the scientific establishment behind the global warming issue has been drawn into the trap of seriously understating the uncertainties associated with the climate problem . this behavior risks destroying science s reputation for honesty . it is this objectivity and honesty which gives science a privileged seat at the table . without this objectivity and honesty , scientists become regarded as another lobbyist group . can climate research get back on track ? can the  uncertainty monster  in climate research , and particularly climate modeling , be acknowledged ? can such study overcome the malthusian bias ( change is bad ; mankind is at fault ) to achieve impartiality ? can the federal role in climate research be scaled back to improve incentives and quality ? optimism , anyone ? there is good news as good science drives out bad . the discrepancy between model - predicted warming and ( lower ) real - world observations has inspired new respect for natural climate variability relative to greenhouse - gas forcing .  although some researchers doubted the existence of a global warming hiatus because of coverage bias , artificial inconsistency , and a change point analysis of instrumental ts records ,  a just - published study at nature.com s scientific reports found ,  it is now accepted that a recent warming deceleration can be clearly observed .  sensitivity estimates  defined as the temperature effect from the enhanced greenhouse effect  have been coming down in the peer - reviewed literature , even to the point when climate economists see a positive externality , not a negative one , from the human influence on climate . ( in technical lingo , the so - called social cost of carbon would be negative . ) such new thinking , long overdue , would shift the mainstream to a new view of the beneficence of plentiful , affordable , reliable energy .\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linecache.getline(str(\"./logical_fallacy/data/test.source\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3ded16d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T15:28:10.472967Z",
     "start_time": "2022-11-24T15:28:10.468795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.isfile(\"/home/crarojasca/Monash/MDFI_misinformation/logical_fallacy/data/test.source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9b2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
